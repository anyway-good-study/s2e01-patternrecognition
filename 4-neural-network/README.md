# Chapter 4 - Neural Network
---

## 환경
- Python 3.5
- tensorflow 1.0.0 + CUDA 8 + cuDNN 8.0

## 예제

### Linear regression

- 실행
```bash
python linear-regression.py
```


- 출력결과
```text
Iter Cost Weight(bias, w0, w1)
0 5.4162 [[ 0.42943311  1.76646829  0.85585797]]
50 0.000439438 [[ 0.04969516  0.98693651  0.98450035]]
100 1.98235e-05 [[ 0.01055486  0.99722546  0.99670798]]
.
.
.
1900 0.0 [[  1.11393931e-07   9.99999940e-01   1.00000000e+00]]
1950 0.0 [[  1.11393931e-07   9.99999940e-01   1.00000000e+00]]
2000 0.0 [[  1.11393931e-07   9.99999940e-01   1.00000000e+00]]
```

- 설명

위 출력결과에서 [[ bias, w0, w1 ]] 이 학습되는 과정을 볼 수 있다.

input/linear-reg-train.txt 학습 데이터를 참고하여 b(bias 는 1), x0, x1 값을 대입하여 확인할 수도 있다.

y = w0*x0 + w1*x1 + b (단, w0=1.0, w1=1.0, b=1.0)

위 함수를 만족하는 W를 구한 것을 알 수 있다.


### Logistic regression

- 실행
```bash
python logistic-regression.py
```

- 출력결과
```text
Iter Cost Weight(bias, w0, w1)
0 1.03547 [[-0.87249863  0.19334999 -0.32115254]]
50 0.470951 [[-1.31579304  0.04680346  0.42432949]]
100 0.424719 [[-1.75767529 -0.01594226  0.59499848]]
.
.
.
1900 0.182975 [[-7.62829638  0.22799857  1.70710874]]
1950 0.181243 [[-7.71938896  0.22919165  1.72613811]]
2000 0.179558 [[-7.80922365  0.23031248  1.74494433]]
=== Prediction ===
[[ 0.02065939]]
[[ 0.88766009]]
[[ 0.16068491  0.83291596]]
```

- 설명

iteration 2,000 번을 수행하였을 때 추정한 hypothesis 인 y = 0.23x0 + 1.74x1 - 7.8 식을 얻을 수 있다.

이 출력값에 대한 sigmoid(y) 값을 취하면 Prediction 값이 올바르게 출력된 것을 확인 할 수 있다.


### Titanic Problem (Kaggle)

